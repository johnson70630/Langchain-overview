{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents\n",
    "\n",
    "more info: https://python.langchain.com/v0.1/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# Advantages of LangChain\\n\\n## 1. Connect LLMs to our own data\\nRecently, there are many use cases in LLMs. However, LLMs may generate answers that do not meet our expectations in some use cases. As a result, we can solve this problem by using LangChain to connect LLMs to our own data. Therefore, LLMs could do referencing from our data.\\n\\n## 2. Combine LLMs on doing different tasks in one use case\\nFor example, we can use GPT-4 to interpret our queries in a specific case and use Claud-3 to response to those queries. By combining two LLMs in different needs, we could have a better model in some use cases.\\n\\n## 3. Split the text to suitable length\\nAs we known, LLMs usually have limited imput token length. LangChain allows us to split a long document into different sections and input to LLMs. Moreover, LLMs can recognize patterns in the texts. Therefore, text-splitting is very efficient.\\n\\n\\n## 4. Control the format of LLM responses\\nWe can provide an example format for the responses we want from LLMs, including specifying the structure, tone, and content. This is known as prompt engineering.\\n\\n## 5. Rich public resources\\nThere are many open-source documents and videos about LangChain concepts and use cases. We could easily access these resources and hands-on learn how to use LangChain.', metadata={'source': 'docs/Advantages_of_Langchain.md'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('docs/Advantages_of_Langchain.md')\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents\n",
    "\n",
    "**Character splitting**\n",
    "\n",
    "`CharacterTextSplitter` is the simplest document splitter; it splits documents into fixed-length text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# Advantages of LangChain\\n\\n## 1. Connect LLMs to our own data\\nRecently, there are many use cases in LLMs. However, LLMs may generate answers that do not meet our expectations in some use cases. As a result, we can solve this problem by using LangChain to connect LLMs to our own data. Therefore, LLMs could do referencing from our data.\\n\\n## 2. Combine LLMs on doing different tasks in one use case\\nFor example, we can use GPT-4 to interpret our queries in a specific case and use Claud-3 to response to those queries. By combining two LLMs in different needs, we could have a better model in some use cases.\\n\\n## 3. Split the text to suitable length\\nAs we known, LLMs usually have limited imput token length. LangChain allows us to split a long document into different sections and input to LLMs. Moreover, LLMs can recognize patterns in the texts. Therefore, text-splitting is very efficient.', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='## 4. Control the format of LLM responses\\nWe can provide an example format for the responses we want from LLMs, including specifying the structure, tone, and content. This is known as prompt engineering.\\n\\n## 5. Rich public resources\\nThere are many open-source documents and videos about LangChain concepts and use cases. We could easily access these resources and hands-on learn how to use LangChain.', metadata={'source': 'docs/Advantages_of_Langchain.md'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code spilitting**\n",
    "\n",
    "The `from_language` function of `RecursiveCharacterTextSplitter` can split code into appropriate text chunks based on the characteristics of the programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='def hello_lanchain():'), Document(page_content='print(\"Hello, Langchain!\")'), Document(page_content='# Call the funciton\\nhello_langchain()')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_lanchain():\n",
    "    print(\"Hello, Langchain!\")\n",
    "\n",
    "# Call the funciton\n",
    "hello_langchain()\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=50, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "print(python_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markdown file splitting**\n",
    "\n",
    "`MarkdownHeaderTextSplitter` can split a Markdown document into chunks based on its paragraph structure and Markdown syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install unstructured\n",
    "# ! pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='This is section 1 ## Section 2  \\nThis is section 2', metadata={'Header 1': 'Chapter 1', 'Header 2': 'Section 1'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_example = \"# Chapter 1\\n\\n  ## Section 1\\n\\n This is section 1 ## Section 2\\n\\n This is section 2\"\n",
    "\n",
    "# Markdown file splitting\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\")\n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "splits = splitter.split_text(markdown_example)\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive character splitting**\n",
    "\n",
    "It uses a set of characters as parameters, trying to split using these characters in sequence until the chunks are small enough. The default character list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. It preserves paragraphs, sentences, and words as much as possible, thus ensuring semantic completeness as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# Advantages of LangChain', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='## 1. Connect LLMs to our own data', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='Recently, there are many use cases in LLMs. However, LLMs may generate answers that do not meet our expectations in some use cases. As a result, we can solve this problem by using LangChain to', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='we can solve this problem by using LangChain to connect LLMs to our own data. Therefore, LLMs could do referencing from our data.', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='## 2. Combine LLMs on doing different tasks in one use case', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='For example, we can use GPT-4 to interpret our queries in a specific case and use Claud-3 to response to those queries. By combining two LLMs in different needs, we could have a better model in some', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='needs, we could have a better model in some use cases.', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='## 3. Split the text to suitable length', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='As we known, LLMs usually have limited imput token length. LangChain allows us to split a long document into different sections and input to LLMs. Moreover, LLMs can recognize patterns in the texts.', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='LLMs can recognize patterns in the texts. Therefore, text-splitting is very efficient.', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='## 4. Control the format of LLM responses', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='We can provide an example format for the responses we want from LLMs, including specifying the structure, tone, and content. This is known as prompt engineering.', metadata={'source': 'docs/Advantages_of_Langchain.md'}), Document(page_content='## 5. Rich public resources\\nThere are many open-source documents and videos about LangChain concepts and use cases. We could easily access these resources and hands-on learn how to use LangChain.', metadata={'source': 'docs/Advantages_of_Langchain.md'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token splitting**\n",
    "\n",
    "Language models, such as OpenAI, have token limits. In API calls, this token limit should not be exceeded. Therefore, splitting text based on the number of tokens is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Advantages of LangChain\\n\\n## 1. Connect LLMs to our own data\\nRecently, there are many use cases in LLMs. However, LLMs may generate answers that do not meet our expectations in some use cases. As a result, we can solve this problem by using LangChain to connect LLMs to our own data. Therefore, LLMs could do referencing from our data.\\n\\n## 2. Combine LLMs on doing different tasks in one use case\\nFor example, we can use GPT-4 to interpret our queries in a specific case and use Claud-3 to response to those queries. By combining two LLMs in different needs, we could have a better model in some use cases.\\n\\n## 3. Split the text to suitable length\\nAs we known, LLMs usually have limited imput token length. LangChain allows us to split a long document into different sections and input to LLMs. Moreover, LLMs can recognize patterns in the texts. Therefore, text-splitting is very efficient.', metadata={'source': 'docs/Advantages_of_Langchain.md'}),\n",
       " Document(page_content='## 4. Control the format of LLM responses\\nWe can provide an example format for the responses we want from LLMs, including specifying the structure, tone, and content. This is known as prompt engineering.\\n\\n## 5. Rich public resources\\nThere are many open-source documents and videos about LangChain concepts and use cases. We could easily access these resources and hands-on learn how to use LangChain.', metadata={'source': 'docs/Advantages_of_Langchain.md'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Document Chunks\n",
    "Embedding models create vector representations of text fragments. This means we can process text in vector space and perform operations such as semantic search to find the most similar text fragments in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embedding = embedding_model.embed_documents(\n",
    "    [\n",
    "        \"Hi!\",\n",
    "        \"I am Johnson.\",\n",
    "        \"I am great!\"\n",
    "    ]\n",
    ")\n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store vector data\n",
    "Vector data storage, also known as vector databases, is responsible for storing vector representations of text embeddings and providing vector retrieval capabilities. Langchain offers several open-source or commercial vector data storage options, including Chroma, FAISS, Pinecone, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "document = text_splitter.split_documents(docs)\n",
    "db = Chroma.from_documents(document, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Advantages of LangChain\n",
      "\n",
      "## 1. Connect LLMs to our own data\n",
      "Recently, there are many use cases in LLMs. However, LLMs may generate answers that do not meet our expectations in some use cases. As a result, we can solve this problem by using LangChain to connect LLMs to our own data. Therefore, LLMs could do referencing from our data.\n",
      "\n",
      "## 2. Combine LLMs on doing different tasks in one use case\n",
      "For example, we can use GPT-4 to interpret our queries in a specific case and use Claud-3 to response to those queries. By combining two LLMs in different needs, we could have a better model in some use cases.\n",
      "\n",
      "## 3. Split the text to suitable length\n",
      "As we known, LLMs usually have limited imput token length. LangChain allows us to split a long document into different sections and input to LLMs. Moreover, LLMs can recognize patterns in the texts. Therefore, text-splitting is very efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is Langchain?\"\n",
    "doc = db.similarity_search(query)\n",
    "print(doc[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
