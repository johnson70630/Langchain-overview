{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback\n",
    "\n",
    "`Callback` is a mechanism provided by `LangChain` that allows us to use hooks at various stages of an LLM application. This is useful for tasks such as logging, monitoring, and streaming. The execution logic for these tasks is defined by the `CallbackHandler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Handlers\n",
    "\n",
    "`StdOutCallbackHandler` is the most basic handler supported by LangChain. It prints all callback information to standard output, which is very useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWho is Andrew Ng?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Andrew Ng is a computer scientist and entrepreneur known for his work in artificial intelligence and machine learning. He co-founded Google Brain and is the co-founder and CEO of Landing AI. Ng is also a professor at Stanford University and has taught courses on machine learning and deep learning. He is a prominent figure in the field of AI and has made significant contributions to the development of AI technologies.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "chat_model = ChatOpenAI(temperature=0.5, model='gpt-3.5-turbo')\n",
    "prompt = PromptTemplate.from_template(\"Who is {name}?\")\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt, callbacks=[handler])\n",
    "chain.run(name=\"Andrew Ng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Handler\n",
    "\n",
    "We can implement a custom callback handler by inheriting from `BaseCallbackHandler`. Here is a simple example, `TimerHandler`, which will track the start and end times of Chain or LLM interactions and calculate the processing time for each interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import time\n",
    "\n",
    "class TimeHandler(BaseCallbackHandler):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.previous_ms = None\n",
    "        self.durations = []\n",
    "\n",
    "    def current_ms(self):\n",
    "        return int(time.time() * 1000 + time.perf_counter() % 1 * 1000)\n",
    "\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs) -> None:\n",
    "        self.previous_ms = self.current_ms()\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs) -> None:\n",
    "        if self.previous_ms:\n",
    "          duration = self.current_ms() - self.previous_ms\n",
    "          self.durations.append(duration/1000)\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs) -> None:\n",
    "        self.previous_ms = self.current_ms()\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs) -> None:\n",
    "        if self.previous_ms:\n",
    "          duration = self.current_ms() - self.previous_ms\n",
    "          self.durations.append(duration/1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The University of Southern California (USC) is located in Los Angeles, California, United States.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.548]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler = TimeHandler()\n",
    "chat_model = ChatOpenAI(temperature=0.5, model='gpt-3.5-turbo')\n",
    "prompt = PromptTemplate.from_template(\"Where is {university}?\")\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt, callbacks=[handler])\n",
    "response = chain.run(university=\"USC\")\n",
    "\n",
    "print(response)\n",
    "handler.durations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
